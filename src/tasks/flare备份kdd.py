"""
FLARE
"""
from lm_eval.base import Task, rf
from lm_eval.metrics import mean
import numpy as np
from .utils import process_text
from .zhutils import process_zhtext
from seqeval.metrics import f1_score as entity_score
from sklearn.metrics import f1_score, matthews_corrcoef, mean_squared_error
#from bart_score import BARTScorer
from metrics.BARTScore.bart_score import BARTScorer
import evaluate
import zipfile
'''
import re
from transformers import  AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
'''

_CITATION = """
@misc{xie2023pixiu,
      title={PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance}, 
      author={Qianqian Xie and Weiguang Han and Xiao Zhang and Yanzhao Lai and Min Peng and Alejandro Lopez-Lira and Jimin Huang},
      year={2023},
      eprint={2306.05443},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
"""


class Classification(Task):
    CALCULATE_MCC = True
    LOWER_CASE = True
    VERSION = 1
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def doc_to_decontamination_query(self, doc):
        return doc["text"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["answer"]

    def process_results(self, doc, results):
        gold: str = doc["choices"][doc["gold"]]
        if self.LOWER_CASE:
            gold = gold.lower()
        ini_result = results[0].strip()
        if self.LOWER_CASE:
            ini_result = ini_result.lower()

        result = None
        for choice in doc["choices"]:
            if self.LOWER_CASE:
                choice = choice.lower()
            if choice in ini_result:
                result = choice
                break
        if result is None:
            result = "missing"

        acc = 1.0 if gold == result else 0.0

        results = {
            "acc": acc,
            "missing": int(result == "missing"),
            "f1": (result, gold),
            "macro_f1": (result, gold),
        }

        if self.CALCULATE_MCC:
            results["mcc"] = (result, gold)

        return results

    def higher_is_better(self):
        metrics = {
            "acc": True,
            "f1": True,
            "macro_f1": True,
            "missing": False,
        }
        if self.CALCULATE_MCC:
            metrics["mcc"] = True
        return metrics

    def weighted_f1(self, items):
        preds, golds = zip(*items)
        labels = list(set(golds))
        preds = np.array(preds)
        golds = np.array(golds)
        f1 = f1_score(golds, preds, average="weighted", labels=labels)
        return f1

    def macro_f1(self, items):
        preds, golds = zip(*items)
        labels = list(set(golds))
        preds = np.array(preds)
        golds = np.array(golds)
        f1 = f1_score(golds, preds, average="macro", labels=labels)
        return f1

    def matthews_corrcoef(self, items):
        preds, golds = zip(*items)
        labels = {label: i for i, label in enumerate(list(set(golds)))}
        preds = [labels.get(pred, -1) for pred in preds]
        golds = [labels.get(gold, -1) for gold in golds]
        return matthews_corrcoef(golds, preds)

    def aggregation(self):
        metrics = {
            "acc": mean,
            "missing": mean,
            "f1": self.weighted_f1,
            "macro_f1": self.macro_f1,
        }
        if self.CALCULATE_MCC:
            metrics["mcc"] = self.matthews_corrcoef
        return metrics

#知识理解
class ThExTask(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/ThEx"
class IDFinReviewsSent(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/indFinReviewSent"



#投资倾向

class IDfinsent(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/indFinsent"


class FIQASA(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/fiqasa"

class FpbTask(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/FPB"




#信用评级

class CCFraudTask(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/ccfraud"


class craccfTask(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/craccf"


class lendingclub(Classification):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/lendingclub"
    CALCULATE_MCC = True








'''
class German(Classification):
    DATASET_PATH = "/gemini/code/PIXIU/PIXIU/data/german"
'''

class SequentialLabeling(Task):
    VERSION = 1
    DATASET_NAME = None
    LMAP = {"O": 0}
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return "\nAnswer: " + doc["answer"]

    def process_results(self, doc, results):
        return {
            "entity_f1": (doc["label"], results[0], doc["token"]),
            "f1": (doc["label"], results[0], doc["token"]),
        }

    def higher_is_better(self):
        return {
            "f1": True,
            "entity_f1": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def process_result(self, pred, gold, tokens):
        format_pred = ["O"] * len(gold)
        for index, pre in enumerate(pred.split("\n")[: len(tokens)]):
            try:
                word, label = pre.split(":")
            except:
                continue
            if word == tokens[index] and label in self.LMAP.keys():
                format_pred[index] = label
        return format_pred

    def entity_f1(self, items):
        golds, preds, tokens = zip(*items)

        list_preds = [
            self.process_result(pred, gold, token)
            for pred, gold, token in zip(preds, golds, tokens)
        ]
        f1 = entity_score(golds, list_preds)
        return f1

    def process_label_result(self, pred, gold, tokens):
        format_pred = [-1] * len(gold)
        for index, pre in enumerate(pred.split("\n")[: len(tokens)]):
            try:
                word, label = pre.split(":")
            except:
                continue
            if word == tokens[index]:
                format_pred[index] = self.LMAP.get(label, -1)
        return format_pred

    def label_f1(self, items):
        golds, preds, tokens = zip(*items)

        list_preds = [
            self.process_label_result(pred, gold, token)
            for pred, gold, token in zip(preds, golds, tokens)
        ]
        list_preds = [item for sublist in list_preds for item in sublist]
        golds = [self.LMAP[item] for sublist in golds for item in sublist]
        f1 = f1_score(golds, list_preds, average="weighted")
        return f1

    def aggregation(self):
        return {
            "entity_f1": self.entity_f1,
            "f1": self.label_f1,
        }

class AbstractiveSummarization(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "rouge1": (doc["answer"], results[0]),
            "rouge2": (doc["answer"], results[0]),
            "rougeL": (doc["answer"], results[0]),
            "bert_score_f1": (doc["answer"], results[0]),
            "bart_score": (doc["answer"], results[0]),
        }

    def higher_is_better(self):
        return {
            "rouge1": True,
            "rouge2": True,
            "rougeL": True,
            "bert_score_f1": True,
            "bart_score": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def rouge_score(self, items):
        golds, preds = zip(*items)
        rouge = evaluate.load("metrics/rouge")
        results = rouge.compute(predictions=preds, references=golds)
        return results

    def rouge1(self, items):
        results = self.rouge_score(items)
        return results["rouge1"]

    def rouge2(self, items):
        results = self.rouge_score(items)
        return results["rouge2"]

    def rougeL(self, items):
        results = self.rouge_score(items)
        return results["rougeL"]
    '''
    def bert_score(self, items):
        if getattr(self, "_cache_bertscore", None) is None:
            golds, preds = zip(*items)
            bertscore = evaluate.load("evaluate-metric/bertscore")
            self._cache_bertscore = bertscore.compute(
                predictions=preds,
                references=golds,
                model_type="D:/胡刚老师/testCODE/PIXIU/pretrain2/bert-base-multilingual-cased",
            )
            return self._cache_bertscore
        else:
            return self._cache_bertscore
    '''



    def bert_score(self, items):
        if getattr(self, "_cache_bertscore", None) is None:
            golds, preds = zip(*items)
            bertscore = evaluate.load("metrics/bertscore")
            self._cache_bertscore = bertscore.compute(
                predictions=preds,
                references=golds,
                # model_type="bert-base-multilingual-cased",
                model_type="D:/胡刚老师/testCODE/PIXIU/pretrain2/bert-base-multilingual-cased",
            )
            return self._cache_bertscore
        else:
            return self._cache_bertscore

    def bert_score_f1(self, items):
        res = self.bert_score(items)
        return sum(res["f1"]) / len(res["f1"])

    def bart_score(self, items):
        golds, preds = zip(*items)
        #bart_scorer = BARTScorer(device="cuda", checkpoint="facebook/bart-large-cnn")
        bart_scorer = BARTScorer(device="cpu", checkpoint="D:/胡刚老师/testCODE/PIXIU/pretrain/bart-large-cnn")
        try:
            with zipfile.ZipFile("D:/胡刚老师/testCODE/PIXIU/src/metrics/BARTScore/bart_score.pth") as zf:
                 print("This is a valid zip file.")
        except zipfile.BadZipFile:
            print("This file is not a valid zip file.")
       # bart_scorer.load(path="D:/胡刚老师/testCODE/PIXIU/src/metrics/BARTScore/bart_score.pth")

        res = bart_scorer.score(srcs=preds, tgts=golds, batch_size=8)
        return sum(res) / len(res)

    def aggregation(self):
        return {
            "rouge1": self.rouge1,
            "rouge2": self.rouge2,
            "rougeL": self.rougeL,
            "bert_score_f1": self.bert_score_f1,
            "bart_score": self.bart_score,
        }







class ExtractiveSummarization(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "rouge1": (doc["label"], doc["text"], results[0]),
            "rouge2": (doc["label"], doc["text"], results[0]),
            "rougeL": (doc["label"], doc["text"], results[0]),
            "bert_score_f1": (doc["label"], doc["text"], results[0]),
            "bart_score": (doc["label"], doc["text"], results[0]),
        }

    def higher_is_better(self):
        return {
            "rouge1": True,
            "rouge2": True,
            "rougeL": True,
            "bert_score_f1": True,
            "bart_score": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def get_sum(self, labels, texts):
        summ = []
        for label, text in zip(labels, texts):
            text = text.split("\n")
            new_text = "\n".join(
                [
                    text[index]
                    for index in range(len(text))
                    if index < len(label) and label[index] == 1
                ]
            )
            summ.append(new_text)
        return summ

    def rouge_score(self, items):
        golds, texts, preds = zip(*items)
        golds = self.get_sum(golds, texts)
        preds = self.get_sum([val.split("\n") for val in preds], texts)
        rouge = evaluate.load("metrics/rouge")
        results = rouge.compute(predictions=preds, references=golds)
        return results

    def rouge1(self, items):
        results = self.rouge_score(items)
        return results["rouge1"]

    def rouge2(self, items):
        results = self.rouge_score(items)
        return results["rouge2"]

    def rougeL(self, items):
        results = self.rouge_score(items)
        return results["rougeL"]

    def bert_score(self, items):
        if getattr(self, "_cache_bertscore", None) is None:
            golds, texts, preds = zip(*items)
            golds = self.get_sum(golds, texts)
            preds = self.get_sum([val.split("\n") for val in preds], texts)

            bertscore = evaluate.load("evaluate-metric/bertscore")
            self._cache_bertscore = bertscore.compute(
                predictions=preds,
                references=golds,
                model_type="bert-base-multilingual-cased",
            )
            return self._cache_bertscore
        else:
            return self._cache_bertscore

    def bert_score_f1(self, items):
        res = self.bert_score(items)
        return sum(res["f1"]) / len(res["f1"])

    def bart_score(self, items):
        golds, texts, preds = zip(*items)
        golds = self.get_sum(golds, texts)
        preds = self.get_sum([val.split("\n") for val in preds], texts)

        bart_scorer = BARTScorer(device="cuda:0", checkpoint="facebook/bart-large-cnn")
        bart_scorer.load(path="src/metrics/BARTScore/bart_score.pth")
        res = bart_scorer.score(srcs=preds, tgts=golds, batch_size=8)
        return sum(res) / len(res)

    def aggregation(self):
        return {
            "rouge1": self.rouge1,
            "rouge2": self.rouge2,
            "rougeL": self.rougeL,
            "bert_score_f1": self.bert_score_f1,
            "bart_score": self.bart_score,
        }




class RelationExtraction(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "precision": (doc["label"], results[0]),
            "recall": (doc["label"], results[0]),
            "f1": (doc["label"], results[0]),
        }

    def higher_is_better(self):
        return {
            "precision": True,
            "recall": True,
            "f1": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def process(self, items):
        golds, preds = zip(*items)

        all_golds = []
        all_preds = []

        for gold, pred in zip(golds, preds):
            all_golds.extend(gold)
            pred = pred.split("\n")
            all_preds.extend(pred)

        return set(all_golds), set(all_preds)

    def precision(self, items):
        golds, preds = self.process(items)
        tp = golds & preds
        prec = len(tp) / len(preds)
        return prec

    def recall(self, items):
        golds, preds = self.process(items)
        tp = golds & preds
        rec = len(tp) / len(golds)
        return rec

    def cal_f1(self, items):
        prec = self.precision(items)
        rec = self.recall(items)
        if prec + rec == 0.0:
            return 0.0
        return 2 * (prec * rec) / (prec + rec)

    def aggregation(self):
        return {
            "precision": self.precision,
            "recall": self.recall,
            "f1": self.cal_f1,
        }



class RelationExtraction(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "precision": (doc["label"], results[0]),
            "recall": (doc["label"], results[0]),
            "f1": (doc["label"], results[0]),
        }

    def higher_is_better(self):
        return {
            "precision": True,
            "recall": True,
            "f1": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def process(self, items):
        golds, preds = zip(*items)

        all_golds = []
        all_preds = []

        for gold, pred in zip(golds, preds):
            all_golds.extend(gold)
            pred = pred.split("\n")
            all_preds.extend(pred)

        return set(all_golds), set(all_preds)

    def precision(self, items):
        golds, preds = self.process(items)
        tp = golds & preds
        prec = len(tp) / len(preds)
        return prec

    def recall(self, items):
        golds, preds = self.process(items)
        tp = golds & preds
        rec = len(tp) / len(golds)
        return rec

    def cal_f1(self, items):
        prec = self.precision(items)
        rec = self.recall(items)
        if prec + rec == 0.0:
            return 0.0
        return 2 * (prec * rec) / (prec + rec)

    def aggregation(self):
        return {
            "precision": self.precision,
            "recall": self.recall,
            "f1": self.cal_f1,
        }


class QA(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["text"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        gold = doc["answer"]

        acc = 1.0 if results[0].strip() == gold else 0.0

        return {
            "acc": acc,
        }

    def higher_is_better(self):
        return {
            "acc": True,
        }

    def aggregation(self):
        return {
            "acc": mean,
        }







class NER(Task):
    VERSION = 1
    DATASET_PATH = "chancefocus/flare-ner"
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["text"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        text = doc["text"]
        pred = process_text(results[0], text)

        return {"entity_f1": (pred, doc["label"], results[0])}

    def higher_is_better(self):
        return {
            "entity_f1": True,
        }

    @classmethod
    def entity_f1(cls, items):
        preds, golds, _ = zip(*items)
        f1 = entity_score(golds, preds)
        return f1

    def aggregation(self):
        return {
            "entity_f1": self.entity_f1,
        }


class FinQA(QA):
    DATASET_PATH = "chancefocus/flare-finqa"

'''这是印度尼西亚语，"kenaikan", "jatuh"意为 "下降'''
'''"tăng", "giảm"'''
'''"เพิ่มขึ้น", "ลดลง"    上升，下降'''
'''Naik", "Turun"'''
'''"Pagtaas", "Pagbagsak"'''
'''["เอาชนะตลาด", "เป็นกลาง", "ไม่ดี"]'''
'''Pagtaas'''
class StockMovement(Classification):
    DATASET_NAME = None
    CALCULATE_MCC = True
    CHOICE_DICT = {
       
        #"tăng":["tăng"],
        #"giảm":["giảm"]

        "kenaikan": ["kenaikan"],
        "jatuh": ["jatuh"],
       #"pagtaas":["pagtaas"],
       #"pagbagsak":["pagbagsak"]


        #"เพิ่มขึ้น", "ลดลง"
        #"เพิ่มขึ้น":["เพิ่มขึ้น"], "Kenaikan", "Jatuh"
        #"ลดลง":["ลดลง"]
        #"เอาชนะตลาด":["เอาชนะตลาด"],
        #"เป็นกลาง":["เป็นกลาง"],
        #"ไม่ดี":["ไม่ดี"]
        #"Kenaikan", "Jatuh"
        #"เพิ่มขึ้น":["เพิ่มขึ้น"],
        #"ลดลง":["ลดลง"]
        #"เอาชนะตลาด", "เป็นกลาง", "ไม่ดี"
        #"เอาชนะตลาด":["เอาชนะตลาด"],
        #"เป็นกลาง":["เป็นกลาง"],
        #"ไม่ดี":["ไม่ดี"],
        #"pagtaas":["pagtaas"],
        #"pagbagsak":["pagbagsak"]
        #"Pagtaas", "Pagbagsak"


        
    }
    
    DEFAULT = "jatuh"

    def process_results(self, doc, results):
        gold: str = doc["choices"][doc["gold"]]
        if self.LOWER_CASE:
            gold = gold.lower()
        ini_result = results[0].strip()
        if self.LOWER_CASE:
            ini_result = ini_result.lower()

        result = None
        for choice in doc["choices"]:
            if self.LOWER_CASE:
                choice = choice.lower()
            if choice in ini_result or any(
                [val in ini_result for val in self.CHOICE_DICT[choice]]
            ):
                result = choice
                break
        if result is None:
            result = self.DEFAULT

        acc = 1.0 if gold == result else 0.0

        results = {
            "acc": acc,
            "missing": int(result == "missing"),
            "f1": (result, gold),
            "macro_f1": (result, gold),
        }

        if self.CALCULATE_MCC:
            results["mcc"] = (result, gold)

        return results







class StockMovementACLTH(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/THacl"
class StockMovementBigDataVI(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/bigdata"
class StockMovementCIKMID(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/cikm"
class StockMovementACLPH(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/PHacl"

class StockMovementACLMY(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/Mayacl"

SM_TASKS = {

    "flare_sm_cikmID": StockMovementCIKMID,
    "flare_sm_bigdataVi": StockMovementBigDataVI,
    "flare_sm_aclPH": StockMovementACLPH,
    "flare_sm_aclMY": StockMovementACLMY,
    "flare_sm_aclTH": StockMovementACLTH,

}



class Headlines(Classification):
    DATASET_PATH = "chancefocus/flare-headlines"

    def process_results(self, doc, results):
        gold = doc["gold"]

        return {
            "avg_f1": (doc["label_type"], int(results[0].strip() != "Yes"), gold, results),
        }

    def higher_is_better(self):
        return {
            "avg_f1": True,
        }

    @classmethod
    def label_avg(cls, items):
        labels, preds, golds, rels = zip(*items)
        label_set = set(labels)
        labels = np.array(labels)
        preds = np.array(preds)
        golds = np.array(golds)
        all_f1s = []
        for l in label_set:
            pds = preds[labels == l]
            gds = golds[labels == l]
            f1 = f1_score(gds, pds, average="weighted", labels=[0, 1])
            all_f1s.append(f1)
        return np.mean(all_f1s)

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def aggregation(self):
        return {
            "avg_f1": self.label_avg,
        }


class FinerOrd(SequentialLabeling):
    DATASET_PATH = "chancefocus/flare-finer-ord"
    LMAP = {
        "O": 0,
        "B-PER": 1,
        "I-PER": 2,
        "B-LOC": 3,
        "I-LOC": 4,
        "B-ORG": 5,
        "I-ORG": 6,
    }


class FOMC(Classification):
    DATASET_PATH = "chancefocus/flare-fomc"
'''
"good": ["yes", "positive"],
"bad": ["no", "negative", "neutral"],
'''
class ThGermanTask(StockMovement):
    DATASET_PATH = "/gemini/code/PIXIU/PIXIU/data/german"
    CHOICE_DICT = {
         "ดี": ["ดี"],
         "ไม่ดี": ["ไม่ดี"],
       }
    DEFAULT = "ดี"

#信用评级
class AustralianTask(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/Australian"
    CHOICE_DICT = {
        "baik": ["baik","yes", "positive"],
        "buruk": ["buruk","no", "negative", "neutral"],
    }
    DEFAULT = "baik"
'''
"baik",  好， "buruk"  坏
'''


class ECTSUM(ExtractiveSummarization):
    DATASET_PATH = "chancefocus/flare-ectsum"


class EDTSUM(AbstractiveSummarization):
    DATASET_PATH = "chancefocus/flare-edtsum"
class FinNATask(AbstractiveSummarization):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/NA"

class IDFinURLSumTask(AbstractiveSummarization):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/IndFinURLSum"
'''
class ConvFin(QA):
    DATASET_PATH = "chancefocus/flare-convfinqa"

    def reformulate_turn_req(self, req, turn_request, turn):QA
        if turn == 0:
            return req
        pre_answers = {f"answer{i}": turn_request[i][0] for i in range(turn)}
        if pre_answers:
            req.args = tuple([req.args[0].format(**pre_answers)] + list(req.args[1:]))
        return req
'''

class TSA(Task):
    VERSION = 1
    DATASET_PATH = "chancefocus/flare-tsa"
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
         # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return "\nAnswer: " + str(doc["answer"])

    def process_results(self, doc, results):
        pred = results[0].split("\n")[0]
        pred = re.findall(r'[0-9]+(?:\.[0-9]+)?', pred)
        missing = 0
        if not pred:
            pred = -100.0
            missing = 1
        else:
            pred = pred[0]
        pred = float(pred)
        return {
                "rmse": (doc["answer"], pred),
                "missing": missing
        }

    def higher_is_better(self):
        return {
            "rmse": False,
        }

    def construct_requests(self, doc, ctx):
        """
        Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": "Answer:"})
        return cont_request

    def rmse(self, items):
        golds, preds = zip(*items)
        fgolds, fpreds = [], []
        for gold, pred in zip(golds, preds):
            if pred == -100.0:
                continue
            fgolds.append(gold)
            fpreds.append(max(min(pred, 1.0), -1.0))
        rmse = mean_squared_error(fgolds, fpreds, squared=True)

        return rmse

    def aggregation(self):
        return {
            "rmse": self.rmse,
            "missing": mean,
         }



























class StockMovementStockA(StockMovement):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/stockA"







class ThcomputerPart(QA):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/ThComputer"

class IndcomputerPart(QA):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/IndComputer"
class FilComputerPart(QA):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/FilComputer"

class MayComputerPart(QA):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/MayComputer"

class VieComputerPart(QA):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/VieComputer"













class PHHeadlines(Headlines):
    DATASET_PATH = "D:/胡刚老师/testCODE/PIXIU/data/Headlines"

    def process_results(self, doc, results):
        gold = doc["gold"]

        return {
            "avg_f1": (doc["answer"], int(results[0] != "Oo"), gold, results),
        }


class ZHBigData(StockMovement):
    DATASET_PATH = "/gemini/code/PIXIU/PIXIU/data/THAcl"
    CHOICE_DICT = {
        "เพิ่มขึ้น": ["เพิ่มขึ้น","ใช่", "เชิงบวก", "ให้กำลังใจ", "ใช่"],
        "ลดลง": ["ลดลง","ไม่ใช่", "เชิงลบ", "เชิงลบ"],
    }
    DEFAULT = "ลดลง"

#"是"翻译为泰语是 "ใช่"，"正面"翻译为泰语是 "เชิงบวก"，"积极"翻译为泰语是 "ให้กำลังใจ"，"肯定的"翻译为泰语是 "ใช่"，"否"翻译为泰语是 "ไม่ใช่"，"负面"翻译为泰语是 "เชิงลบ"，"消极"翻译为泰语是 "เชิงลบ"。
class ZHACL(ZHBigData):
    DATASET_PATH = "ChanceFocus/flare-zh-acl"


class ZHCIKM(ZHBigData):
    DATASET_PATH = "ChanceFocus/flare-zh-cikm"
class THACL(ZHBigData):
    DATASET_PATH = "/gemini/code/PIXIU/PIXIU/data/THAcl"

class ZHFinQAE(QA):
    DATASET_PATH = "ChanceFocus/flare-zh-finqa"

'''
class ZHConvFinQA(ConvFinQA):
    DATASET_PATH = "ChanceFocus/flare-zh-convfinqa"


class ZHxuanyuan(ZHFinEval):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/xuanyuan"

class ZHdisc(ZHFinQA):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/disc"

class ZHcompany(ZH19CCKS):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/company"

class ZHproduct(ZH19CCKS):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/product"

class ZHsentiment(Classification):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/sentiment"
'''
class ZHsuggestion(RelationExtraction):
    DATASET_PATH = "/content/drive/MyDrive/PIXIU/data/suggestion"

    def process_results(self, doc, results):
        return {
            "precision": (doc["answer"], results[0]),
            "recall": (doc["answer"], results[0]),
            "f1": (doc["answer"], results[0]),
            "conise_sim": (doc["answer"], results[0]),
        }

    def higher_is_better(self):
        return {
            "precision": True,
            "recall": True,
            "f1": True,
            "conise_sim": True,
        }
    
    def conise_sim(self, items):
      model_name = "BAAI/bge-base-zh-v1.5"  # bge-zh-v1.5 模型
      tokenizer = AutoTokenizer.from_pretrained(model_name)
      model = AutoModel.from_pretrained(model_name)
      golds, preds = self.process(items)
      sentence1 = str(golds)
      sentence2 = str(preds)

      # 使用模型和分词器将句子转换为向量
      inputs1 = tokenizer(sentence1, return_tensors="pt", padding=True, truncation=True)
      inputs2 = tokenizer(sentence2, return_tensors="pt", padding=True, truncation=True)

      # 获取模型输出
      outputs1 = model(**inputs1)
      outputs2 = model(**inputs2)

      # 获取模型输出的向量表示
      vector1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()[0]
      vector2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()[0]

      # 计算余弦相似度
      cosine_sim = cosine_similarity([vector1], [vector2])[0][0]
      cosine_sim = float(cosine_sim)

      return cosine_sim

    def aggregation(self):
        return {
            "precision": self.precision,
            "recall": self.recall,
            "f1": self.cal_f1,
            "conise_sim": self.conise_sim,
        }
